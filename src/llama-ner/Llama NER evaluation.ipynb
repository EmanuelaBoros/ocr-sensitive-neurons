{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94f3cf3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-prod': 0, 'B-time': 1, 'O': 2, 'B-time.date.abs': 3, 'B-comp.qualifier': 4, 'B-pers': 5, 'I-comp.name': 6, 'B-comp.title': 7, 'B-org.adm': 8, 'B-loc.add.phys': 9, 'B-comp.function': 10, 'I-prod.media': 11, 'I-org.adm': 12, 'I-loc.adm.reg': 13, 'I-comp.demonym': 14, 'I-prod': 15, 'I-pers.ind': 16, 'B-loc.add.elec': 17, 'I-org': 18, 'I-loc.phys.hydro': 19, 'I-comp.function': 20, 'B-comp.demonym': 21, 'I-loc.add.phys': 22, 'B-loc.oro': 23, 'B-pers.ind.articleauthor': 24, 'I-pers.ind.articleauthor': 25, 'B-comp.name': 26, 'I-org.ent.pressagency': 27, 'I-pers.coll': 28, 'B-loc.adm.sup': 29, 'B-pers.ind': 30, 'I-loc.oro': 31, 'I-loc.adm.town': 32, 'I-loc.phys.geo': 33, 'I-comp.title': 34, 'B-org.ent': 35, 'B-loc.fac': 36, 'B-loc.unk': 37, 'I-prod.doctr': 38, 'B-loc.adm.town': 39, 'I-loc.adm.nat': 40, 'I-pers': 41, 'B-org': 42, 'I-loc.fac': 43, 'I-org.ent': 44, 'B-pers.coll': 45, 'I-time.date.abs': 46, 'I-loc.add.elec': 47, 'B-prod.media': 48, 'I-loc': 49, 'I-time': 50, 'I-loc.adm.sup': 51, 'B-loc.phys.geo': 52, 'B-loc.adm.nat': 53, 'I-loc.unk': 54, 'B-prod.doctr': 55, 'I-comp.qualifier': 56, 'B-org.ent.pressagency': 57, 'B-loc': 58, 'B-loc.adm.reg': 59, 'B-loc.phys.hydro': 60}\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from modeling_llama import LlamaForTokenClassification\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "import pprint\n",
    "\n",
    "\n",
    "def load_ontonotesv5():\n",
    "    ret = {}\n",
    "    for split_name in [\"train\", \"dev\", \"test\"]:\n",
    "        data = []\n",
    "        with open(f\"./data/NER/ontonotesv5/{split_name}.jsonl\", \"r\") as reader:\n",
    "            for line in reader:\n",
    "                data.append(json.loads(line))\n",
    "        ret[split_name] = Dataset.from_list(data)\n",
    "    return DatasetDict(ret)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_hipe():\n",
    "    \"\"\"\n",
    "        Load HIPE 2020 dataset\n",
    "\n",
    "        {'annotations': ['B-pers', 'O', 'B-pers.ind', 'O', 'O'], 'token': 'Etienne'}\n",
    "\n",
    "    CoNLL:\n",
    "    {'id': '13408',\n",
    "    'tokens': ['Egypt', 'police', 'catch', 'ancient', 'manuscript', 'thieves', '.'],\n",
    "    'pos_tags': [22, 21, 21, 16, 21, 24, 7], 'chunk_tags': [11, 12, 12, 12, 12, 12, 0],\n",
    "    'ner_tags': [5, 0, 0, 0, 0, 0, 0]}\n",
    "\n",
    "        :return:\n",
    "    \"\"\"\n",
    "    ret = {}\n",
    "    label_set = set()  # Set to collect all unique labels\n",
    "    for split_name in [\"train\", \"dev\", \"test\"]:\n",
    "        data = []\n",
    "        with open(\n",
    "            f\"data/hipe2020/fr/HIPE-2022-v2.1-hipe2020-{split_name}-fr.tsv\",\n",
    "            \"r\",\n",
    "            encoding=\"utf-8\",\n",
    "        ) as reader:\n",
    "            document = {}\n",
    "            tokens = []\n",
    "            annotations = {}\n",
    "            for i in range(1, 6):\n",
    "                annotations[f\"ner_tags{i}\"] = []\n",
    "            for line in reader:\n",
    "                if \"NE-COARSE-LIT\" in line:\n",
    "                    columns = line.strip().split(\"\\t\")\n",
    "                    continue\n",
    "                if line.startswith(\"# \"):  # Metadata or comments\n",
    "                    if line.startswith(\"# hipe2022:document_id\"):\n",
    "                        document = {\n",
    "                            \"doc_id\": line.strip().split(\"=\")[1].strip(),\n",
    "                            \"metadata\": {},\n",
    "                        }\n",
    "                    elif line.strip().startswith(\"# \"):\n",
    "                        key, value = line.strip().split(\"=\", 1)\n",
    "                        document[\"metadata\"][key.strip()] = value.strip()\n",
    "                else:\n",
    "                    parts = line.strip().split(\"\\t\")\n",
    "                    if (line.strip() == \"\") or (\"EndOfSentence\" in line):\n",
    "                        if tokens:\n",
    "                            sentence = {\"tokens\": tokens}\n",
    "                            for i in range(1, 6):\n",
    "                                sentence[f\"ner_tags{i}\"] = annotations[f\"ner_tags{i}\"]\n",
    "                            data.append(sentence)\n",
    "                            tokens = []\n",
    "                            annotations = {}\n",
    "                            for i in range(1, 6):\n",
    "                                annotations[f\"ner_tags{i}\"] = []\n",
    "                    else:\n",
    "                        tokens.append(parts[0])\n",
    "                        if len(parts) > 6:\n",
    "                            for i in range(1, 6):\n",
    "                                annotations[f\"ner_tags{i}\"].append(parts[i])\n",
    "                                if parts[i] != \"_\":  # Assuming \"_\" means no label\n",
    "                                    label_set.add(parts[i])\n",
    "\n",
    "            if tokens:  # Catch last sentence\n",
    "                sentence = {\"tokens\": tokens}\n",
    "                for i in range(1, 6):\n",
    "                    sentence[f\"ner_tags{i}\"] = annotations[f\"ner_tags{i}\"]\n",
    "                data.append(sentence)\n",
    "                annotations = {}\n",
    "                for i in range(1, 6):\n",
    "                    annotations[f\"ner_tags{i}\"] = []\n",
    "        ret[split_name] = data\n",
    "\n",
    "    # Generate label2id dictionary\n",
    "    label2id = {label: idx for idx, label in enumerate(label_set)}\n",
    "    for split_name in [\"train\", \"dev\", \"test\"]:\n",
    "        for idx, sentence in enumerate(ret[split_name]):\n",
    "            for i in range(1, 6):\n",
    "                ret[split_name][idx][f\"ner_tags{i}\"] = [\n",
    "                    label2id[label] for label in sentence[f\"ner_tags{i}\"]\n",
    "                ]\n",
    "        ret[split_name] = Dataset.from_list(ret[split_name])\n",
    "    return (\n",
    "        DatasetDict(ret),\n",
    "        label2id,\n",
    "    )  # Adjust DatasetDict according to your data handling\n",
    "\n",
    "if len(sys.argv) != 3:\n",
    "    print(\"usage python %.py task model_size\")\n",
    "    sys.exit()\n",
    "\n",
    "task, model_size = \"hipe2020\", \"7b\"\n",
    "\n",
    "\n",
    "learning_rate = 1e-4\n",
    "max_length = 64\n",
    "if model_size == \"7b\":\n",
    "    model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "    lora_r = 12\n",
    "elif model_size == \"13b\":\n",
    "    model_id = \"NousResearch/Llama-2-13b-hf\"\n",
    "    lora_r = 12\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "if task == \"conll2003\":\n",
    "    ds = load_dataset(\"conll2003\")\n",
    "    label2id = {\n",
    "        \"O\": 0,\n",
    "        \"B-PER\": 1,\n",
    "        \"I-PER\": 2,\n",
    "        \"B-ORG\": 3,\n",
    "        \"I-ORG\": 4,\n",
    "        \"B-LOC\": 5,\n",
    "        \"I-LOC\": 6,\n",
    "        \"B-MISC\": 7,\n",
    "        \"I-MISC\": 8,\n",
    "    }\n",
    "elif task == \"hipe2020\":\n",
    "    ds, label2id = load_hipe()\n",
    "    print(label2id)\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "label_list = list(label2id.keys())  # ds[\"train\"].features[f\"ner_tags\"].feature.names\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        padding=\"longest\",\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags1\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(\n",
    "            batch_index=i\n",
    "        )  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif (\n",
    "                word_idx != previous_word_idx\n",
    "            ):  # Only label the first token of a given word.\n",
    "                try:\n",
    "                    label_ids.append(label[word_idx])\n",
    "                except:\n",
    "                    import pdb\n",
    "\n",
    "                    pdb.set_trace()\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    # logger.info(f\"Results: {results}\")\n",
    "    # Create a PrettyPrinter instance\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "    # Use the PrettyPrinter to display the results\n",
    "    logger.info(\"Results:\")\n",
    "    pp.pprint(results)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "843b04e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags1', 'ner_tags2', 'ner_tags3', 'ner_tags4', 'ner_tags5'],\n",
       "        num_rows: 5679\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['tokens', 'ner_tags1', 'ner_tags2', 'ner_tags3', 'ner_tags4', 'ner_tags5'],\n",
       "        num_rows: 1229\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags1', 'ner_tags2', 'ner_tags3', 'ner_tags4', 'ner_tags5'],\n",
       "        num_rows: 1446\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9dc7fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a66b450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1159136b116433cbd129a1319d22fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c42bc8eb2c4f4688a44a5d1dd2865639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForTokenClassification were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,541,373 || all params: 6,614,134,906 || trainable%: 0.09889990290439957\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa38071f8b384cb588da049b4f859e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5679 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c44c68f80541d5818ce949c9da4c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81114add23744f4ea38d53276b194486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1446 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LlamaForTokenClassification.from_pretrained(\n",
    "    model_id, num_labels=len(label2id), id2label=id2label, label2id=label2id\n",
    ").bfloat16()\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS,\n",
    "    inference_mode=False,\n",
    "    r=lora_r,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "tokenized_ds = ds.map(tokenize_and_align_labels, batched=True)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfe51bd2",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 11.92 GiB of which 39.44 MiB is free. Including non-PyTorch memory, this process has 11.88 GiB memory in use. Of the allocated memory 11.45 GiB is allocated by PyTorch, and 289.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 19\u001b[0m\n\u001b[1;32m      2\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      4\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      5\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrained_models/finetune_all_llama2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     push_to_hub\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 19\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     20\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     21\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     22\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_ds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     23\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_ds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     24\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     25\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m     26\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     29\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m/scratch/eboros/.conda/envs/impresso_annotation_updated/lib/python3.11/site-packages/transformers/trainer.py:514\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;66;03m# Bnb Quantized models doesn't support `.to` operation.\u001b[39;00m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_model_on_device\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES\n\u001b[1;32m    513\u001b[0m ):\n\u001b[0;32m--> 514\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_move_model_to_device(model, args\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    516\u001b[0m \u001b[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_model_parallel:\n",
      "File \u001b[0;32m/scratch/eboros/.conda/envs/impresso_annotation_updated/lib/python3.11/site-packages/transformers/trainer.py:757\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_move_model_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, device):\n\u001b[0;32m--> 757\u001b[0m     model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mparallel_mode \u001b[38;5;241m==\u001b[39m ParallelMode\u001b[38;5;241m.\u001b[39mTPU \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtie_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/scratch/eboros/.conda/envs/impresso_annotation_updated/lib/python3.11/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
      "File \u001b[0;32m/scratch/eboros/.conda/envs/impresso_annotation_updated/lib/python3.11/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/eboros/.conda/envs/impresso_annotation_updated/lib/python3.11/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 802 (4 times)]\u001b[0m\n",
      "File \u001b[0;32m/scratch/eboros/.conda/envs/impresso_annotation_updated/lib/python3.11/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/eboros/.conda/envs/impresso_annotation_updated/lib/python3.11/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/scratch/eboros/.conda/envs/impresso_annotation_updated/lib/python3.11/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 11.92 GiB of which 39.44 MiB is free. Including non-PyTorch memory, this process has 11.88 GiB memory in use. Of the allocated memory 11.45 GiB is allocated by PyTorch, and 289.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 1\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"trained_models/finetune_all_llama2\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epochs,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b1f6eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712cc4e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1773b45d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfc0dde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b393ee13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ed3a83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9475db8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf74375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b3cade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9faf4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84fd709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b54c441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b4fca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308bb085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68944733",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece5f199",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7003da5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
