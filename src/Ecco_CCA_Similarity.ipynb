{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d3e8316-6769-44e1-b522-cb4b35fc4541",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/jalammar/ecco/blob/main/notebooks/Ecco_CCA_Similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c05bfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install ecco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbc08cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce13a8ac7b9440185d37a568734eece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ed4965cb8ff4d41855f5c822ca7cd29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "260e6e349c2547f3839add48e7c53c8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc211c79cd2f465b820671675c9c3f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5Model\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5Model.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc483e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Model(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "354fdc72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a72d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bb0758f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06cda7d458014e68983067e746216abd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-v0.1')\n",
    "model = AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-v0.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59cc27ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56475fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8278ff144fc34b4780817f7a84357a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e66bd203e7748698161ea45091cb1b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb180bd0c5554b3fac59bd7c8d766ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6fed916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4aca4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Now I ask you: what can be expected of man since he is a being endowed with strange qualities? Shower upon him every earthly blessing, drown him in a sea of happiness, so that nothing but bubbles of bliss can be seen on the surface; give him economic prosperity, such that he should have nothing else to do but sleep, eat cakes and busy himself with the continuation of his species, and even then out of sheer ingratitude, sheer spite, man would play you some nasty trick. He would even risk his cakes and would deliberately desire the most fatal rubbish, the most uneconomical absurdity, simply to introduce into all this positive good sense his fatal fantastic element. It is just his fantastic dreams, his vulgar folly that he will desire to retain, simply in order to prove to himself--as though that were so necessary-- that men still are men and not the keys of a piano, which the laws of nature threaten to control so completely that soon one will be able to desire nothing but by the calendar. And that is not all: even if man really were nothing but a piano-key, even if this were proved to him by natural science and mathematics, even then he would not become reasonable, but would purposely do something perverse out of simple ingratitude, simply to gain his point. And if he does not find means he will contrive destruction and chaos, will contrive sufferings of all sorts, only to gain his point! He will launch a curse upon the world, and as only man can curse (it is his privilege, the primary distinction between him and other animals), may be by his curse alone he will attain his object--that is, convince himself that he is a man and not a piano-key!\n",
    "'''\n",
    "\n",
    "encoded = tokenizer([text], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf376a68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm()\n",
      "        (post_attention_layernorm): MistralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n",
      "model MistralModel(\n",
      "  (embed_tokens): Embedding(32000, 4096)\n",
      "  (layers): ModuleList(\n",
      "    (0-31): 32 x MistralDecoderLayer(\n",
      "      (self_attn): MistralSdpaAttention(\n",
      "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (rotary_emb): MistralRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): MistralMLP(\n",
      "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): MistralRMSNorm()\n",
      "      (post_attention_layernorm): MistralRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): MistralRMSNorm()\n",
      ")\n",
      "model.embed_tokens Embedding(32000, 4096)\n",
      "model.layers ModuleList(\n",
      "  (0-31): 32 x MistralDecoderLayer(\n",
      "    (self_attn): MistralSdpaAttention(\n",
      "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (rotary_emb): MistralRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): MistralMLP(\n",
      "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): MistralRMSNorm()\n",
      "    (post_attention_layernorm): MistralRMSNorm()\n",
      "  )\n",
      ")\n",
      "model.layers.0 MistralDecoderLayer(\n",
      "  (self_attn): MistralSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm()\n",
      "  (post_attention_layernorm): MistralRMSNorm()\n",
      ")\n",
      "model.layers.0.self_attn MistralSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): MistralRotaryEmbedding()\n",
      ")\n",
      "model.layers.0.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.0.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.0.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.0.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.0.self_attn.rotary_emb MistralRotaryEmbedding()\n",
      "model.layers.0.mlp MistralMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.0.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.0.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.0.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.0.mlp.act_fn SiLU()\n",
      "model.layers.0.input_layernorm MistralRMSNorm()\n",
      "model.layers.0.post_attention_layernorm MistralRMSNorm()\n",
      "model.layers.1 MistralDecoderLayer(\n",
      "  (self_attn): MistralSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm()\n",
      "  (post_attention_layernorm): MistralRMSNorm()\n",
      ")\n",
      "model.layers.1.self_attn MistralSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): MistralRotaryEmbedding()\n",
      ")\n",
      "model.layers.1.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.1.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.1.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.1.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.1.self_attn.rotary_emb MistralRotaryEmbedding()\n",
      "model.layers.1.mlp MistralMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.1.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.1.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.1.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.1.mlp.act_fn SiLU()\n",
      "model.layers.1.input_layernorm MistralRMSNorm()\n",
      "model.layers.1.post_attention_layernorm MistralRMSNorm()\n",
      "model.layers.2 MistralDecoderLayer(\n",
      "  (self_attn): MistralSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm()\n",
      "  (post_attention_layernorm): MistralRMSNorm()\n",
      ")\n",
      "model.layers.2.self_attn MistralSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): MistralRotaryEmbedding()\n",
      ")\n",
      "model.layers.2.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.2.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.2.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.2.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.2.self_attn.rotary_emb MistralRotaryEmbedding()\n",
      "model.layers.2.mlp MistralMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.2.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.2.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.2.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.2.mlp.act_fn SiLU()\n",
      "model.layers.2.input_layernorm MistralRMSNorm()\n",
      "model.layers.2.post_attention_layernorm MistralRMSNorm()\n",
      "model.layers.3 MistralDecoderLayer(\n",
      "  (self_attn): MistralSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm()\n",
      "  (post_attention_layernorm): MistralRMSNorm()\n",
      ")\n",
      "model.layers.3.self_attn MistralSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): MistralRotaryEmbedding()\n",
      ")\n",
      "model.layers.3.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.3.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.3.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.3.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.3.self_attn.rotary_emb MistralRotaryEmbedding()\n",
      "model.layers.3.mlp MistralMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.3.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.3.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.3.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.3.mlp.act_fn SiLU()\n",
      "model.layers.3.input_layernorm MistralRMSNorm()\n",
      "model.layers.3.post_attention_layernorm MistralRMSNorm()\n",
      "model.layers.4 MistralDecoderLayer(\n",
      "  (self_attn): MistralSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm()\n",
      "  (post_attention_layernorm): MistralRMSNorm()\n",
      ")\n",
      "model.layers.4.self_attn MistralSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): MistralRotaryEmbedding()\n",
      ")\n",
      "model.layers.4.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.4.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.4.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.4.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.4.self_attn.rotary_emb MistralRotaryEmbedding()\n",
      "model.layers.4.mlp MistralMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.4.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.4.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.4.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.4.mlp.act_fn SiLU()\n",
      "model.layers.4.input_layernorm MistralRMSNorm()\n",
      "model.layers.4.post_attention_layernorm MistralRMSNorm()\n",
      "model.layers.5 MistralDecoderLayer(\n",
      "  (self_attn): MistralSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm()\n",
      "  (post_attention_layernorm): MistralRMSNorm()\n",
      ")\n",
      "model.layers.5.self_attn MistralSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): MistralRotaryEmbedding()\n",
      ")\n",
      "model.layers.5.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.5.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.5.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.5.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.5.self_attn.rotary_emb MistralRotaryEmbedding()\n",
      "model.layers.5.mlp MistralMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.5.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.5.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.5.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.5.mlp.act_fn SiLU()\n",
      "model.layers.5.input_layernorm MistralRMSNorm()\n",
      "model.layers.5.post_attention_layernorm MistralRMSNorm()\n",
      "model.layers.6 MistralDecoderLayer(\n",
      "  (self_attn): MistralSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm()\n",
      "  (post_attention_layernorm): MistralRMSNorm()\n",
      ")\n",
      "model.layers.6.self_attn MistralSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): MistralRotaryEmbedding()\n",
      ")\n",
      "model.layers.6.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.6.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.6.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.6.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.6.self_attn.rotary_emb MistralRotaryEmbedding()\n",
      "model.layers.6.mlp MistralMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.6.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.6.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.6.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.6.mlp.act_fn SiLU()\n",
      "model.layers.6.input_layernorm MistralRMSNorm()\n",
      "model.layers.6.post_attention_layernorm MistralRMSNorm()\n",
      "model.layers.7 MistralDecoderLayer(\n",
      "  (self_attn): MistralSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm()\n",
      "  (post_attention_layernorm): MistralRMSNorm()\n",
      ")\n",
      "model.layers.7.self_attn MistralSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): MistralRotaryEmbedding()\n",
      ")\n",
      "model.layers.7.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.7.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.7.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.7.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.7.self_attn.rotary_emb MistralRotaryEmbedding()\n",
      "model.layers.7.mlp MistralMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.7.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.7.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.7.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.7.mlp.act_fn SiLU()\n",
      "model.layers.7.input_layernorm MistralRMSNorm()\n",
      "model.layers.7.post_attention_layernorm MistralRMSNorm()\n",
      "model.layers.8 MistralDecoderLayer(\n",
      "  (self_attn): MistralSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm()\n",
      "  (post_attention_layernorm): MistralRMSNorm()\n",
      ")\n",
      "model.layers.8.self_attn MistralSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): MistralRotaryEmbedding()\n",
      ")\n",
      "model.layers.8.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.8.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.8.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.8.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.8.self_attn.rotary_emb MistralRotaryEmbedding()\n",
      "model.layers.8.mlp MistralMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.8.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.8.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.8.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.8.mlp.act_fn SiLU()\n",
      "model.layers.8.input_layernorm MistralRMSNorm()\n",
      "model.layers.8.post_attention_layernorm MistralRMSNorm()\n",
      "model.layers.9 MistralDecoderLayer(\n",
      "  (self_attn): MistralSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm()\n",
      "  (post_attention_layernorm): MistralRMSNorm()\n",
      ")\n",
      "model.layers.9.self_attn MistralSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): MistralRotaryEmbedding()\n",
      ")\n",
      "model.layers.9.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.9.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.9.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.9.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.9.self_attn.rotary_emb MistralRotaryEmbedding()\n",
      "model.layers.9.mlp MistralMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.9.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.9.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.9.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.9.mlp.act_fn SiLU()\n",
      "model.layers.9.input_layernorm MistralRMSNorm()\n",
      "model.layers.9.post_attention_layernorm MistralRMSNorm()\n",
      "model.layers.10 MistralDecoderLayer(\n",
      "  (self_attn): MistralSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm()\n",
      "  (post_attention_layernorm): MistralRMSNorm()\n",
      ")\n",
      "model.layers.10.self_attn MistralSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): MistralRotaryEmbedding()\n",
      ")\n",
      "model.layers.10.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.10.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.10.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.10.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.10.self_attn.rotary_emb MistralRotaryEmbedding()\n",
      "model.layers.10.mlp MistralMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.10.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.10.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.10.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.10.mlp.act_fn SiLU()\n",
      "model.layers.10.input_layernorm MistralRMSNorm()\n",
      "model.layers.10.post_attention_layernorm MistralRMSNorm()\n",
      "model.layers.11 MistralDecoderLayer(\n",
      "  (self_attn): MistralSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm()\n",
      "  (post_attention_layernorm): MistralRMSNorm()\n",
      ")\n",
      "model.layers.11.self_attn MistralSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): MistralRotaryEmbedding()\n",
      ")\n",
      "model.layers.11.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.11.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.11.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.11.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.11.self_attn.rotary_emb MistralRotaryEmbedding()\n",
      "model.layers.11.mlp MistralMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.11.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.11.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.11.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.11.mlp.act_fn SiLU()\n",
      "model.layers.11.input_layernorm MistralRMSNorm()\n",
      "model.layers.11.post_attention_layernorm MistralRMSNorm()\n",
      "model.layers.12 MistralDecoderLayer(\n",
      "  (self_attn): MistralSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm()\n",
      "  (post_attention_layernorm): MistralRMSNorm()\n",
      ")\n",
      "model.layers.12.self_attn MistralSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): MistralRotaryEmbedding()\n",
      ")\n",
      "model.layers.12.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.12.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.12.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.12.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.12.self_attn.rotary_emb MistralRotaryEmbedding()\n",
      "model.layers.12.mlp MistralMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.12.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.12.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.12.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.12.mlp.act_fn SiLU()\n",
      "model.layers.12.input_layernorm MistralRMSNorm()\n",
      "model.layers.12.post_attention_layernorm MistralRMSNorm()\n",
      "model.layers.13 MistralDecoderLayer(\n",
      "  (self_attn): MistralSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm()\n",
      "  (post_attention_layernorm): MistralRMSNorm()\n",
      ")\n",
      "model.layers.13.self_attn MistralSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): MistralRotaryEmbedding()\n",
      ")\n",
      "model.layers.13.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.13.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.13.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.13.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.13.self_attn.rotary_emb MistralRotaryEmbedding()\n",
      "model.layers.13.mlp MistralMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.13.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.13.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.13.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.13.mlp.act_fn SiLU()\n",
      "model.layers.13.input_layernorm MistralRMSNorm()\n",
      "model.layers.13.post_attention_layernorm MistralRMSNorm()\n",
      "model.layers.14 MistralDecoderLayer(\n",
      "  (self_attn): MistralSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm()\n",
      "  (post_attention_layernorm): MistralRMSNorm()\n",
      ")\n",
      "model.layers.14.self_attn MistralSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): MistralRotaryEmbedding()\n",
      ")\n",
      "model.layers.14.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.14.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.14.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.14.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.14.self_attn.rotary_emb MistralRotaryEmbedding()\n",
      "model.layers.14.mlp MistralMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.14.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.14.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.14.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.14.mlp.act_fn SiLU()\n",
      "model.layers.14.input_layernorm MistralRMSNorm()\n",
      "model.layers.14.post_attention_layernorm MistralRMSNorm()\n",
      "model.layers.15 MistralDecoderLayer(\n",
      "  (self_attn): MistralSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm()\n",
      "  (post_attention_layernorm): MistralRMSNorm()\n",
      ")\n",
      "model.layers.15.self_attn MistralSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): MistralRotaryEmbedding()\n",
      ")\n",
      "model.layers.15.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.15.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.15.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.15.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.15.self_attn.rotary_emb MistralRotaryEmbedding()\n",
      "model.layers.15.mlp MistralMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.15.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.15.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.15.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.15.mlp.act_fn SiLU()\n",
      "model.layers.15.input_layernorm MistralRMSNorm()\n",
      "model.layers.15.post_attention_layernorm MistralRMSNorm()\n",
      "model.layers.16 MistralDecoderLayer(\n",
      "  (self_attn): MistralSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm()\n",
      "  (post_attention_layernorm): MistralRMSNorm()\n",
      ")\n",
      "model.layers.16.self_attn MistralSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): MistralRotaryEmbedding()\n",
      ")\n",
      "model.layers.16.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.16.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.16.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.16.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.16.self_attn.rotary_emb MistralRotaryEmbedding()\n",
      "model.layers.16.mlp MistralMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.16.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.16.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.16.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.16.mlp.act_fn SiLU()\n",
      "model.layers.16.input_layernorm MistralRMSNorm()\n",
      "model.layers.16.post_attention_layernorm MistralRMSNorm()\n",
      "model.layers.17 MistralDecoderLayer(\n",
      "  (self_attn): MistralSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm()\n",
      "  (post_attention_layernorm): MistralRMSNorm()\n",
      ")\n",
      "model.layers.17.self_attn MistralSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): MistralRotaryEmbedding()\n",
      ")\n",
      "model.layers.17.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.17.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.17.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.17.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.17.self_attn.rotary_emb MistralRotaryEmbedding()\n",
      "model.layers.17.mlp MistralMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.17.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.17.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.17.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.17.mlp.act_fn SiLU()\n",
      "model.layers.17.input_layernorm MistralRMSNorm()\n",
      "model.layers.17.post_attention_layernorm MistralRMSNorm()\n",
      "model.layers.18 MistralDecoderLayer(\n",
      "  (self_attn): MistralSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm()\n",
      "  (post_attention_layernorm): MistralRMSNorm()\n",
      ")\n",
      "model.layers.18.self_attn MistralSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): MistralRotaryEmbedding()\n",
      ")\n",
      "model.layers.18.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.18.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.18.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.18.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.18.self_attn.rotary_emb MistralRotaryEmbedding()\n",
      "model.layers.18.mlp MistralMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.18.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.18.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.18.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.18.mlp.act_fn SiLU()\n",
      "model.layers.18.input_layernorm MistralRMSNorm()\n",
      "model.layers.18.post_attention_layernorm MistralRMSNorm()\n",
      "model.layers.19 MistralDecoderLayer(\n",
      "  (self_attn): MistralSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm()\n",
      "  (post_attention_layernorm): MistralRMSNorm()\n",
      ")\n",
      "model.layers.19.self_attn MistralSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): MistralRotaryEmbedding()\n",
      ")\n",
      "model.layers.19.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.19.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.19.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.19.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.19.self_attn.rotary_emb MistralRotaryEmbedding()\n",
      "model.layers.19.mlp MistralMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.19.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.19.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.19.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.19.mlp.act_fn SiLU()\n",
      "model.layers.19.input_layernorm MistralRMSNorm()\n",
      "model.layers.19.post_attention_layernorm MistralRMSNorm()\n",
      "model.layers.20 MistralDecoderLayer(\n",
      "  (self_attn): MistralSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm()\n",
      "  (post_attention_layernorm): MistralRMSNorm()\n",
      ")\n",
      "model.layers.20.self_attn MistralSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): MistralRotaryEmbedding()\n",
      ")\n",
      "model.layers.20.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.20.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.20.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.20.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.20.self_attn.rotary_emb MistralRotaryEmbedding()\n",
      "model.layers.20.mlp MistralMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.20.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.20.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.20.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.20.mlp.act_fn SiLU()\n",
      "model.layers.20.input_layernorm MistralRMSNorm()\n",
      "model.layers.20.post_attention_layernorm MistralRMSNorm()\n",
      "model.layers.21 MistralDecoderLayer(\n",
      "  (self_attn): MistralSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm()\n",
      "  (post_attention_layernorm): MistralRMSNorm()\n",
      ")\n",
      "model.layers.21.self_attn MistralSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): MistralRotaryEmbedding()\n",
      ")\n",
      "model.layers.21.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.21.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.21.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.21.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.21.self_attn.rotary_emb MistralRotaryEmbedding()\n",
      "model.layers.21.mlp MistralMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.21.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.21.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.21.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.21.mlp.act_fn SiLU()\n",
      "model.layers.21.input_layernorm MistralRMSNorm()\n",
      "model.layers.21.post_attention_layernorm MistralRMSNorm()\n",
      "model.layers.22 MistralDecoderLayer(\n",
      "  (self_attn): MistralSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm()\n",
      "  (post_attention_layernorm): MistralRMSNorm()\n",
      ")\n",
      "model.layers.22.self_attn MistralSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): MistralRotaryEmbedding()\n",
      ")\n",
      "model.layers.22.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.22.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.22.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.22.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.22.self_attn.rotary_emb MistralRotaryEmbedding()\n",
      "model.layers.22.mlp MistralMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.22.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.22.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.22.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.22.mlp.act_fn SiLU()\n",
      "model.layers.22.input_layernorm MistralRMSNorm()\n",
      "model.layers.22.post_attention_layernorm MistralRMSNorm()\n",
      "model.layers.23 MistralDecoderLayer(\n",
      "  (self_attn): MistralSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm()\n",
      "  (post_attention_layernorm): MistralRMSNorm()\n",
      ")\n",
      "model.layers.23.self_attn MistralSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): MistralRotaryEmbedding()\n",
      ")\n",
      "model.layers.23.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.23.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.23.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.23.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.23.self_attn.rotary_emb MistralRotaryEmbedding()\n",
      "model.layers.23.mlp MistralMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.23.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.23.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.23.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.23.mlp.act_fn SiLU()\n",
      "model.layers.23.input_layernorm MistralRMSNorm()\n",
      "model.layers.23.post_attention_layernorm MistralRMSNorm()\n",
      "model.layers.24 MistralDecoderLayer(\n",
      "  (self_attn): MistralSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm()\n",
      "  (post_attention_layernorm): MistralRMSNorm()\n",
      ")\n",
      "model.layers.24.self_attn MistralSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): MistralRotaryEmbedding()\n",
      ")\n",
      "model.layers.24.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.24.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.24.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.24.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.24.self_attn.rotary_emb MistralRotaryEmbedding()\n",
      "model.layers.24.mlp MistralMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.24.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.24.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.24.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.24.mlp.act_fn SiLU()\n",
      "model.layers.24.input_layernorm MistralRMSNorm()\n",
      "model.layers.24.post_attention_layernorm MistralRMSNorm()\n",
      "model.layers.25 MistralDecoderLayer(\n",
      "  (self_attn): MistralSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm()\n",
      "  (post_attention_layernorm): MistralRMSNorm()\n",
      ")\n",
      "model.layers.25.self_attn MistralSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): MistralRotaryEmbedding()\n",
      ")\n",
      "model.layers.25.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.25.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.25.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.25.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.25.self_attn.rotary_emb MistralRotaryEmbedding()\n",
      "model.layers.25.mlp MistralMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.25.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.25.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.25.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.25.mlp.act_fn SiLU()\n",
      "model.layers.25.input_layernorm MistralRMSNorm()\n",
      "model.layers.25.post_attention_layernorm MistralRMSNorm()\n",
      "model.layers.26 MistralDecoderLayer(\n",
      "  (self_attn): MistralSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm()\n",
      "  (post_attention_layernorm): MistralRMSNorm()\n",
      ")\n",
      "model.layers.26.self_attn MistralSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): MistralRotaryEmbedding()\n",
      ")\n",
      "model.layers.26.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.26.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.26.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.26.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.26.self_attn.rotary_emb MistralRotaryEmbedding()\n",
      "model.layers.26.mlp MistralMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.26.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.26.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.26.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.26.mlp.act_fn SiLU()\n",
      "model.layers.26.input_layernorm MistralRMSNorm()\n",
      "model.layers.26.post_attention_layernorm MistralRMSNorm()\n",
      "model.layers.27 MistralDecoderLayer(\n",
      "  (self_attn): MistralSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm()\n",
      "  (post_attention_layernorm): MistralRMSNorm()\n",
      ")\n",
      "model.layers.27.self_attn MistralSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): MistralRotaryEmbedding()\n",
      ")\n",
      "model.layers.27.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.27.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.27.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.27.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.27.self_attn.rotary_emb MistralRotaryEmbedding()\n",
      "model.layers.27.mlp MistralMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.27.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.27.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.27.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.27.mlp.act_fn SiLU()\n",
      "model.layers.27.input_layernorm MistralRMSNorm()\n",
      "model.layers.27.post_attention_layernorm MistralRMSNorm()\n",
      "model.layers.28 MistralDecoderLayer(\n",
      "  (self_attn): MistralSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm()\n",
      "  (post_attention_layernorm): MistralRMSNorm()\n",
      ")\n",
      "model.layers.28.self_attn MistralSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): MistralRotaryEmbedding()\n",
      ")\n",
      "model.layers.28.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.28.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.28.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.28.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.28.self_attn.rotary_emb MistralRotaryEmbedding()\n",
      "model.layers.28.mlp MistralMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.28.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.28.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.28.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.28.mlp.act_fn SiLU()\n",
      "model.layers.28.input_layernorm MistralRMSNorm()\n",
      "model.layers.28.post_attention_layernorm MistralRMSNorm()\n",
      "model.layers.29 MistralDecoderLayer(\n",
      "  (self_attn): MistralSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm()\n",
      "  (post_attention_layernorm): MistralRMSNorm()\n",
      ")\n",
      "model.layers.29.self_attn MistralSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): MistralRotaryEmbedding()\n",
      ")\n",
      "model.layers.29.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.29.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.29.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.29.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.29.self_attn.rotary_emb MistralRotaryEmbedding()\n",
      "model.layers.29.mlp MistralMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.29.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.29.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.29.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.29.mlp.act_fn SiLU()\n",
      "model.layers.29.input_layernorm MistralRMSNorm()\n",
      "model.layers.29.post_attention_layernorm MistralRMSNorm()\n",
      "model.layers.30 MistralDecoderLayer(\n",
      "  (self_attn): MistralSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm()\n",
      "  (post_attention_layernorm): MistralRMSNorm()\n",
      ")\n",
      "model.layers.30.self_attn MistralSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): MistralRotaryEmbedding()\n",
      ")\n",
      "model.layers.30.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.30.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.30.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.30.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.30.self_attn.rotary_emb MistralRotaryEmbedding()\n",
      "model.layers.30.mlp MistralMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.30.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.30.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.30.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.30.mlp.act_fn SiLU()\n",
      "model.layers.30.input_layernorm MistralRMSNorm()\n",
      "model.layers.30.post_attention_layernorm MistralRMSNorm()\n",
      "model.layers.31 MistralDecoderLayer(\n",
      "  (self_attn): MistralSdpaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm()\n",
      "  (post_attention_layernorm): MistralRMSNorm()\n",
      ")\n",
      "model.layers.31.self_attn MistralSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): MistralRotaryEmbedding()\n",
      ")\n",
      "model.layers.31.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.31.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.31.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.31.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.31.self_attn.rotary_emb MistralRotaryEmbedding()\n",
      "model.layers.31.mlp MistralMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.31.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.31.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.31.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.31.mlp.act_fn SiLU()\n",
      "model.layers.31.input_layernorm MistralRMSNorm()\n",
      "model.layers.31.post_attention_layernorm MistralRMSNorm()\n",
      "model.norm MistralRMSNorm()\n",
      "lm_head Linear(in_features=4096, out_features=32000, bias=False)\n"
     ]
    }
   ],
   "source": [
    "# Print all layers and their names\n",
    "for name, module in model.named_modules():\n",
    "    print(name, module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fe6cef9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a tokenization example\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m tokenized \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "text = \"This is a tokenization example\"\n",
    "tokenized = tokenizer.tokenize(text) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c35009a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>\n",
      "Now\n",
      "I\n",
      "ask\n",
      "you\n",
      ":\n",
      "what\n",
      "can\n",
      "be\n",
      "expected\n",
      "of\n",
      "man\n",
      "since\n",
      "he\n",
      "is\n",
      "a\n",
      "being\n",
      "end\n",
      "owed\n",
      "with\n",
      "strange\n",
      "qualities\n",
      "?\n",
      "Sh\n",
      "ower\n",
      "upon\n",
      "him\n",
      "every\n",
      "earth\n",
      "ly\n",
      "blessing\n",
      ",\n",
      "d\n",
      "rown\n",
      "him\n",
      "in\n",
      "a\n",
      "sea\n",
      "of\n",
      "happiness\n",
      ",\n",
      "so\n",
      "that\n",
      "nothing\n",
      "but\n",
      "bub\n",
      "bles\n",
      "of\n",
      "bl\n",
      "iss\n",
      "can\n",
      "be\n",
      "seen\n",
      "on\n",
      "the\n",
      "surface\n",
      ";\n",
      "give\n",
      "him\n",
      "economic\n",
      "prosper\n",
      "ity\n",
      ",\n",
      "such\n",
      "that\n",
      "he\n",
      "should\n",
      "have\n",
      "nothing\n",
      "else\n",
      "to\n",
      "do\n",
      "but\n",
      "sleep\n",
      ",\n",
      "eat\n",
      "c\n",
      "akes\n",
      "and\n",
      "busy\n",
      "himself\n",
      "with\n",
      "the\n",
      "continu\n",
      "ation\n",
      "of\n",
      "his\n",
      "species\n",
      ",\n",
      "and\n",
      "even\n",
      "then\n",
      "out\n",
      "of\n",
      "sheer\n",
      "ing\n",
      "rat\n",
      "itude\n",
      ",\n",
      "sheer\n",
      "spite\n",
      ",\n",
      "man\n",
      "would\n",
      "play\n",
      "you\n",
      "some\n",
      "nasty\n",
      "trick\n",
      ".\n",
      "He\n",
      "would\n",
      "even\n",
      "risk\n",
      "his\n",
      "c\n",
      "akes\n",
      "and\n",
      "would\n",
      "deliberately\n",
      "desire\n",
      "the\n",
      "most\n",
      "fatal\n",
      "rub\n",
      "b\n",
      "ish\n",
      ",\n",
      "the\n",
      "most\n",
      "une\n",
      "conom\n",
      "ical\n",
      "absurd\n",
      "ity\n",
      ",\n",
      "simply\n",
      "to\n",
      "introduce\n",
      "into\n",
      "all\n",
      "this\n",
      "positive\n",
      "good\n",
      "sense\n",
      "his\n",
      "fatal\n",
      "fantastic\n",
      "element\n",
      ".\n",
      "It\n",
      "is\n",
      "just\n",
      "his\n",
      "fantastic\n",
      "dreams\n",
      ",\n",
      "his\n",
      "vul\n",
      "gar\n",
      "fol\n",
      "ly\n",
      "that\n",
      "he\n",
      "will\n",
      "desire\n",
      "to\n",
      "retain\n",
      ",\n",
      "simply\n",
      "in\n",
      "order\n",
      "to\n",
      "prove\n",
      "to\n",
      "himself\n",
      "--\n",
      "as\n",
      "though\n",
      "that\n",
      "were\n",
      "so\n",
      "necessary\n",
      "--\n",
      "that\n",
      "men\n",
      "still\n",
      "are\n",
      "men\n",
      "and\n",
      "not\n",
      "the\n",
      "keys\n",
      "of\n",
      "a\n",
      "piano\n",
      ",\n",
      "which\n",
      "the\n",
      "laws\n",
      "of\n",
      "nature\n",
      "threat\n",
      "en\n",
      "to\n",
      "control\n",
      "so\n",
      "completely\n",
      "that\n",
      "soon\n",
      "one\n",
      "will\n",
      "be\n",
      "able\n",
      "to\n",
      "desire\n",
      "nothing\n",
      "but\n",
      "by\n",
      "the\n",
      "calendar\n",
      ".\n",
      "And\n",
      "that\n",
      "is\n",
      "not\n",
      "all\n",
      ":\n",
      "even\n",
      "if\n",
      "man\n",
      "really\n",
      "were\n",
      "nothing\n",
      "but\n",
      "a\n",
      "piano\n",
      "-\n",
      "key\n",
      ",\n",
      "even\n",
      "if\n",
      "this\n",
      "were\n",
      "proved\n",
      "to\n",
      "him\n",
      "by\n",
      "natural\n",
      "science\n",
      "and\n",
      "mathemat\n",
      "ics\n",
      ",\n",
      "even\n",
      "then\n",
      "he\n",
      "would\n",
      "not\n",
      "become\n",
      "reasonable\n",
      ",\n",
      "but\n",
      "would\n",
      "pur\n",
      "pos\n",
      "ely\n",
      "do\n",
      "something\n",
      "per\n",
      "verse\n",
      "out\n",
      "of\n",
      "simple\n",
      "ing\n",
      "rat\n",
      "itude\n",
      ",\n",
      "simply\n",
      "to\n",
      "gain\n",
      "his\n",
      "point\n",
      ".\n",
      "And\n",
      "if\n",
      "he\n",
      "does\n",
      "not\n",
      "find\n",
      "means\n",
      "he\n",
      "will\n",
      "cont\n",
      "rive\n",
      "destruction\n",
      "and\n",
      "chaos\n",
      ",\n",
      "will\n",
      "cont\n",
      "rive\n",
      "suffer\n",
      "ings\n",
      "of\n",
      "all\n",
      "sorts\n",
      ",\n",
      "only\n",
      "to\n",
      "gain\n",
      "his\n",
      "point\n",
      "!\n",
      "He\n",
      "will\n",
      "launch\n",
      "a\n",
      "curse\n",
      "upon\n",
      "the\n",
      "world\n",
      ",\n",
      "and\n",
      "as\n",
      "only\n",
      "man\n",
      "can\n",
      "curse\n",
      "(\n",
      "it\n",
      "is\n",
      "his\n",
      "privilege\n",
      ",\n",
      "the\n",
      "primary\n",
      "distinction\n",
      "between\n",
      "him\n",
      "and\n",
      "other\n",
      "animals\n",
      "),\n",
      "may\n",
      "be\n",
      "by\n",
      "his\n",
      "curse\n",
      "alone\n",
      "he\n",
      "will\n",
      "att\n",
      "ain\n",
      "his\n",
      "object\n",
      "--\n",
      "that\n",
      "is\n",
      ",\n",
      "convince\n",
      "himself\n",
      "that\n",
      "he\n",
      "is\n",
      "a\n",
      "man\n",
      "and\n",
      "not\n",
      "a\n",
      "piano\n",
      "-\n",
      "key\n",
      "!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in encoded['input_ids'][0]: print(tokenizer.decode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d37c83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55290353-1c0f-4778-abd8-bb4c09969e1a",
   "metadata": {},
   "source": [
    "Load Ecco and BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "228061f6-2cfc-47ea-9357-6789c81745d1",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ecco\n",
    "lm = ecco.from_pretrained('gpt2', gpu=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d1f126-e938-42f6-8e8c-fb97f79a2b74",
   "metadata": {},
   "source": [
    "Let's give BERT a passage of text to proccess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1abd436-50bf-4722-b113-73eaa795020d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html lang=\"en\">\n",
       "<script src=\"https://requirejs.org/docs/release/2.3.6/minified/require.js\"></script>\n",
       "<script>\n",
       "    var ecco_url = 'https://storage.googleapis.com/ml-intro/ecco/'\n",
       "    //var ecco_url = 'http://localhost:8000/'\n",
       "\n",
       "    if (window.ecco === undefined) window.ecco = {}\n",
       "\n",
       "    // Setup the paths of the script we'll be using\n",
       "    requirejs.config({\n",
       "        urlArgs: \"bust=\" + (new Date()).getTime(),\n",
       "        nodeRequire: require,\n",
       "        paths: {\n",
       "            d3: \"https://d3js.org/d3.v6.min\", // This is only for use in setup.html and basic.html\n",
       "            \"d3-array\": \"https://d3js.org/d3-array.v2.min\",\n",
       "            jquery: \"https://code.jquery.com/jquery-3.5.1.min\",\n",
       "            ecco: ecco_url + 'js/0.0.6/ecco-bundle.min',\n",
       "            xregexp: 'https://cdnjs.cloudflare.com/ajax/libs/xregexp/3.2.0/xregexp-all.min'\n",
       "        }\n",
       "    });\n",
       "\n",
       "    // Add the css file\n",
       "    //requirejs(['d3'],\n",
       "    //    function (d3) {\n",
       "    //        d3.select('#css').attr('href', ecco_url + 'html/styles.css')\n",
       "    //    })\n",
       "\n",
       "    console.log('Ecco initialize!!')\n",
       "\n",
       "    // returns a 'basic' object. basic.init() selects the html div we'll be\n",
       "    // rendering the html into, adds styles.css to the document.\n",
       "    define('basic', ['d3'],\n",
       "        function (d3) {\n",
       "            return {\n",
       "                init: function (viz_id = null) {\n",
       "                    if (viz_id == null) {\n",
       "                        viz_id = \"viz_\" + Math.round(Math.random() * 10000000)\n",
       "                    }\n",
       "                    // Select the div rendered below, change its id\n",
       "                    const div = d3.select('#basic').attr('id', viz_id),\n",
       "                        div_parent = d3.select('#' + viz_id).node().parentNode\n",
       "\n",
       "                    // Link to CSS file\n",
       "                    d3.select(div_parent).insert('link')\n",
       "                        .attr('rel', 'stylesheet')\n",
       "                        .attr('type', 'text/css')\n",
       "                        .attr('href', ecco_url + 'html/0.0.2/styles.css')\n",
       "\n",
       "                    return viz_id\n",
       "                }\n",
       "            }\n",
       "        }, function (err) {\n",
       "            console.log(err);\n",
       "        }\n",
       "    )\n",
       "</script>\n",
       "\n",
       "<head>\n",
       "    <link id='css' rel=\"stylesheet\" type=\"text/css\">\n",
       "</head>\n",
       "<div id=\"basic\"></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "         requirejs(['basic', 'ecco'], function(basic, ecco){\n",
       "            const viz_id = basic.init()\n",
       "\n",
       "            ecco.renderOutputSequence({\n",
       "                parentDiv: viz_id,\n",
       "                data: {'tokens': [{'token': 'En', 'token_id': 4834, 'type': 'input'}, {'token': 'vert', 'token_id': 9421, 'type': 'input'}, {'token': 'u', 'token_id': 84, 'type': 'input'}, {'token': 'd', 'token_id': 288, 'type': 'input'}, {'token': \"'\", 'token_id': 6, 'type': 'input'}, {'token': 'un', 'token_id': 403, 'type': 'input'}, {'token': 'gr', 'token_id': 1036, 'type': 'input'}, {'token': 'ac', 'token_id': 330, 'type': 'input'}, {'token': 'ie', 'token_id': 494, 'type': 'input'}, {'token': 'ux', 'token_id': 2821, 'type': 'input'}, {'token': 'arr', 'token_id': 5240, 'type': 'input'}, {'token': '', 'token_id': 25792, 'type': 'input'}, {'token': 't', 'token_id': 83, 'type': 'input'}, {'token': 'du', 'token_id': 7043, 'type': 'input'}, {'token': 'Gou', 'token_id': 46531, 'type': 'input'}, {'token': 'vern', 'token_id': 933, 'type': 'input'}, {'token': 'ement', 'token_id': 972, 'type': 'input'}, {'token': 'en', 'token_id': 551, 'type': 'input'}, {'token': 'date', 'token_id': 3128, 'type': 'input'}, {'token': 'du', 'token_id': 7043, 'type': 'input'}, {'token': '3', 'token_id': 513, 'type': 'input'}, {'token': 'oct', 'token_id': 19318, 'type': 'input'}, {'token': 'ob', 'token_id': 672, 'type': 'input'}, {'token': 're', 'token_id': 260, 'type': 'input'}, {'token': ',', 'token_id': 11, 'type': 'input'}, {'token': 'et', 'token_id': 2123, 'type': 'input'}, {'token': 'ensu', 'token_id': 29084, 'type': 'input'}, {'token': 'ite', 'token_id': 578, 'type': 'input'}, {'token': 'de', 'token_id': 390, 'type': 'input'}, {'token': 'la', 'token_id': 8591, 'type': 'input'}, {'token': 'direction', 'token_id': 4571, 'type': 'input'}, {'token': 'don', 'token_id': 836, 'type': 'input'}, {'token': 'n', 'token_id': 77, 'type': 'input'}, {'token': 'e', 'token_id': 22161, 'type': 'input'}, {'token': 'le', 'token_id': 443, 'type': 'input'}, {'token': '35', 'token_id': 3439, 'type': 'input'}, {'token': 'no', 'token_id': 645, 'type': 'input'}, {'token': 've', 'token_id': 303, 'type': 'input'}, {'token': 'mb', 'token_id': 2022, 'type': 'input'}, {'token': 're', 'token_id': 260, 'type': 'input'}, {'token': 'd', 'token_id': 288, 'type': 'input'}, {'token': 'ern', 'token_id': 1142, 'type': 'input'}, {'token': 'ier', 'token_id': 959, 'type': 'input'}, {'token': 'par', 'token_id': 1582, 'type': 'input'}, {'token': 'l', 'token_id': 300, 'type': 'input'}, {'token': \"'\", 'token_id': 6, 'type': 'input'}, {'token': 'hon', 'token_id': 24130, 'type': 'input'}, {'token': 'orable', 'token_id': 10475, 'type': 'input'}, {'token': 'cour', 'token_id': 1093, 'type': 'input'}, {'token': 'de', 'token_id': 390, 'type': 'input'}, {'token': 'justice', 'token_id': 5316, 'type': 'input'}, {'token': 'du', 'token_id': 7043, 'type': 'input'}, {'token': 'Val', 'token_id': 3254, 'type': 'input'}, {'token': '-', 'token_id': 12, 'type': 'input'}, {'token': 'de', 'token_id': 2934, 'type': 'input'}, {'token': '-', 'token_id': 12, 'type': 'input'}, {'token': 'Tra', 'token_id': 15721, 'type': 'input'}, {'token': 'vers', 'token_id': 690, 'type': 'input'}, {'token': ',', 'token_id': 11, 'type': 'input'}, {'token': 'le', 'token_id': 443, 'type': 'input'}, {'token': 's', 'token_id': 264, 'type': 'input'}, {'token': 'ie', 'token_id': 494, 'type': 'input'}, {'token': 'ur', 'token_id': 333, 'type': 'input'}, {'token': 'Daniel', 'token_id': 7806, 'type': 'input'}, {'token': 'Me', 'token_id': 2185, 'type': 'input'}, {'token': 'uron', 'token_id': 44372, 'type': 'input'}, {'token': ',', 'token_id': 11, 'type': 'input'}, {'token': 'ma', 'token_id': 17266, 'type': 'input'}, {'token': '', 'token_id': 34803, 'type': 'input'}, {'token': 'tre', 'token_id': 33945, 'type': 'input'}, {'token': 'char', 'token_id': 1149, 'type': 'input'}, {'token': 'pent', 'token_id': 16923, 'type': 'input'}, {'token': 'ier', 'token_id': 959, 'type': 'input'}, {'token': ',', 'token_id': 11, 'type': 'input'}, {'token': 'bourgeois', 'token_id': 19749, 'type': 'input'}, {'token': 'de', 'token_id': 390, 'type': 'input'}, {'token': 'Ne', 'token_id': 3169, 'type': 'input'}, {'token': 'uch', 'token_id': 794, 'type': 'input'}, {'token': '', 'token_id': 22940, 'type': 'input'}, {'token': 'tel', 'token_id': 37524, 'type': 'input'}, {'token': ',', 'token_id': 11, 'type': 'input'}, {'token': 'en', 'token_id': 551, 'type': 'input'}, {'token': 'sa', 'token_id': 473, 'type': 'input'}, {'token': 'qual', 'token_id': 4140, 'type': 'input'}, {'token': 'it', 'token_id': 43816, 'type': 'input'}, {'token': 'de', 'token_id': 390, 'type': 'input'}, {'token': 't', 'token_id': 256, 'type': 'input'}, {'token': 'ute', 'token_id': 1133, 'type': 'input'}, {'token': 'ur', 'token_id': 333, 'type': 'input'}, {'token': 'jur', 'token_id': 8174, 'type': 'input'}, {'token': 'id', 'token_id': 312, 'type': 'input'}, {'token': 'iqu', 'token_id': 1557, 'type': 'input'}, {'token': 'ement', 'token_id': 972, 'type': 'input'}, {'token': '', 'token_id': 38251, 'type': 'input'}, {'token': 'tab', 'token_id': 8658, 'type': 'input'}, {'token': 'li', 'token_id': 4528, 'type': 'input'}, {'token': 'aux', 'token_id': 27506, 'type': 'input'}, {'token': '.', 'token_id': 13, 'type': 'input'}]},\n",
       "                tokenization_config: {\"token_prefix\": \"\\u0120\", \"partial_token_prefix\": \"\"}\n",
       "            })\n",
       "         }, function (err) {\n",
       "            console.log(err);\n",
       "        })"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<OutputSeq>"
      ],
      "text/plain": [
       "<ecco.output.OutputSeq at 0x7fc93fd67390>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''En vertu d'un gracieux arrt du Gouvernement en date du 3 octobre, et ensuite de la direction donne le 35 novembre dernier par l'honorable cour de justice du Val-de-Travers, le sieur Daniel Meuron, matre charpentier, bourgeois de Neuchtel, en sa qualit de tuteur juridiquement tabli aux.'''\n",
    "inputs = lm.tokenizer([text], return_tensors=\"pt\")\n",
    "output = lm(inputs)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470b6fa8-1f45-42e6-afa8-ba6fbe576bb4",
   "metadata": {},
   "source": [
    "the `output` variable now contains the result of BERT processing the passge of text. The property `output.decoder_hidden_states` contains the hidden states after each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c97a987-c239-4f51-8f39-f4f00dfa9464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((768, 98), (768, 98), 12)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states = output._get_encoder_hidden_states()\n",
    "embed = output.embedding_states.detach().numpy()[0,:,:].T\n",
    "hidden_state_layer = [layer.detach().numpy()[0,:,:].T for layer in hidden_states]\n",
    "embed.shape, hidden_state_layer[0].shape, len(hidden_state_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ee4085-a697-4753-98b8-f930b4abcad0",
   "metadata": {},
   "source": [
    "`embed` now contains the embeddings of the inputs. Its dimensions are (embed_dim, number of tokens). \n",
    "`hidden_state_layer` has the outputs of each of the model's 6 layers. The output of each layer is (embed_dim, number of tokens).\n",
    "\n",
    "This is how to calculate the cka similarity score between the embeddings layer and the output of the first layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08ebb7da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((768, 98), numpy.ndarray)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.shape, type(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9aaf9dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((768, 98), numpy.ndarray)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_state_layer[0].shape, type(hidden_state_layer[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fe8fa2b-39b7-4a04-bf4e-c62cffe3f2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8560358046122208"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ecco import analysis\n",
    "analysis.cka(embed, hidden_state_layer[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831da973-63bc-4a53-8022-8017ee82af57",
   "metadata": {},
   "source": [
    "When we compare the embeddings with the output of the second layer, we see less similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d428f519-fb48-402d-8bea-962783fe36de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08467288924944512"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis.cka(embed, hidden_state_layer[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823e071c-ccc2-458e-b792-8d451bd48e41",
   "metadata": {},
   "source": [
    "And so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8621e099-a768-4e17-86e6-6281c4fe4a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08470355600732336"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis.cka(embed, hidden_state_layer[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2de6f2-fe10-4672-ae51-cfdfe2382be2",
   "metadata": {},
   "source": [
    "We can try with `cca`, `svcca` and `pwcca`. But we need to choose a subset of the neurons because these methods require more tokens than neurons (and advise 10x as many tokens as neurons to get a proper similarity score). \n",
    "\n",
    "Let's compare the similarities of the first 50 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc3f50a6-eaa0-498e-896a-11d333d7fb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCA - Embed vs. layer 0: 0.8861507394810895\n",
      "CCA - Embed vs. layer 1: 0.8146598036124624\n"
     ]
    }
   ],
   "source": [
    "print(\"CCA - Embed vs. layer 0:\", analysis.cca(embed[:50,:], hidden_state_layer[0][:50,:]))\n",
    "print(\"CCA - Embed vs. layer 1:\", analysis.cca(embed[:50,:], hidden_state_layer[1][:50,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebb5d691-af03-4d0c-9282-c5338671df12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVCCA - Embed vs. layer 0: 0.7185418895664394\n",
      "SVCCA - Embed vs. layer 1: 0.5814988167043111\n"
     ]
    }
   ],
   "source": [
    "print(\"SVCCA - Embed vs. layer 0:\", analysis.svcca(embed[:50,:], hidden_state_layer[0][:50,:]))\n",
    "print(\"SVCCA - Embed vs. layer 1:\", analysis.svcca(embed[:50,:], hidden_state_layer[1][:50,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b3e47b3-e5a6-47ce-8964-29eb086e29ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PWCCA - Embed vs. layer 0: 0.8989839143133123\n",
      "PWCCA - Embed vs. layer 1: 0.8351718541862796\n"
     ]
    }
   ],
   "source": [
    "print(\"PWCCA - Embed vs. layer 0:\", analysis.pwcca(embed[:50,:], hidden_state_layer[0][:50,:]))\n",
    "print(\"PWCCA - Embed vs. layer 1:\", analysis.pwcca(embed[:50,:], hidden_state_layer[1][:50,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e4e49a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
